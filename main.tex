\documentclass[twocolumn]{article}

\usepackage{amsmath}

\usepackage{bm}

\usepackage{physics}

\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{epstopdf}
\graphicspath{{img/}}

\everymath{\displaystyle}

\begin{document}

\title{Restricted Boltzmann Machines}
\author{Giancarlo Fissore}
\date{May 2017}
\maketitle

\begin{abstract}
  Abstract text
\end{abstract}

\section{Introduction}

\section{Training}
A Restricted Boltzmann Machine (RBM) is a model for neural networks which basically consists in a bipartite graph with a layer of hidden units \(h_i\) and a layer of visible units \(v_i\). The units in one layer are not connected among them but are connected to all the units in the other layer, as shown in fig. x. We restrict our treatment to the case of binary units \(h_i,v_i = 0,1\). Drawing a comparison between RBMs and spin models in statistical physics, we can define the following \textit{energy function}

\begin{equation}
E(\textbf{h},\textbf{v}) = - \sum_i a_i v_i - \sum_j b_j h_j - \sum_{i,j} v_i w_{ij} h_j
\end{equation}

where \(a_i\) and \(b_i\) are \textit{external fields} acting respectively on the visible and hidden units. The probability of a certain configuration is then given by the Boltzmann measure (taking \(\beta = 1\))

\begin{equation}
P(\textbf{h},\textbf{v}) = \frac{e^{-E(\textbf{h},\textbf{v})}}{Z}
\end{equation}

where \(Z = \sum_{\textbf{h},\textbf{v}} e^{-E(\textbf{h},\textbf{v})}\) is the \textit{partition function}. The probabilities of activations for visible and hidden units can be simply computed to be

\begin{align}
P(v_i = 1 | \textbf{h}) &  = \frac{1}{1+e^{-a_i - \sum_{j} w_{ij} h_j}} \nonumber \\
& = \sigma \left(a_i + \sum_{j} w_{ij} h_j \right) \\
P(h_j = 1 | \textbf{v}) & = \frac{1}{1+e^{b_j + \sum_i w_{ij} v_i}} \nonumber \\
& = \sigma \left(b_j + \sum_i w_{ij} v_i \right)
\end{align}

\begin{figure}
  \centering
  \includegraphics{rbm.png}
  \caption{bipartite structure of a RBM}
\end{figure}

We see that the usual neuron activation function (the sigmoid function, which we indicated by \(\sigma\)) comes out naturally by choosing binary units. Moreover, we can now understand the importance of the external fields: when negative, they drive the activation of a certain unit by setting a threshold that has to be overcome by the opposite layer through the couplings \(w_{ij}\).

What we are interested in is the probability for the visible units, which is the layer we use to represent data. It can be easily defined as

\begin{align}
P(\textbf{v}) &  = \sum_{\textbf{h}} P(\textbf{h},\textbf{v}) \nonumber \\
& = \frac{e^{-F(\textbf{v})}}{Z}, \quad Z = \sum_{\textbf{v}} e^{-F(\textbf{v})}
\end{align}

where, further exploiting the parallel with spin models, we have introduced the \textit{free energy}

\begin{align}
F(\textbf{v}) & = -\log \sum_{\textbf{h}} e^{-E(\textbf{h},\textbf{v})} \nonumber \\
& = -\sum_i a_i v_i -\sum_j \log \sum_{h_j} e^{h_j \left( b_j + \sum_i w_{ij} v_i \right)} \nonumber \\
& = -\sum_i a_i v_i -\sum_j \log \left( 1 +  e^{\left( b_j + \sum_i w_{ij} v_i \right)} \right)
\end{align}

To use the RBM as a generative model, we want to maximize \(P(\textbf{v}\)) (and thus minimize the free energy) for the samples belonging to the training set. Once this is done, we can sample the equilibrium configurations of the RBM to obtain samples which are generated according to the probability distribution of the training data. The algorithms used to train and sample RBMs are usually based on Monte Carlo methods. In particular, the most effective algorithms for training are \textit{k-steps contrastive divergence} (CDk) \cite{Hinton_CD}  and \textit{persistent contrastive divergence} (PCD) \cite{PCD}. 

\section{Spectral analysis}
The structure of the samples that a RBM  is able to generate must be in some way encoded into the external fields \(\textbf{a},\textbf{b}\) and the weights matrix \(\textbf{W}\), as these are the set of quantities which constitute the RBM itself. For what concerns the visible field values \(a_i\), these are used to make sure that the visible unit \(i\) is activated with a probability \(p_i\) given by the proportion of training samples in which unit \(i\) is active (i.e. where its value is 1). No learning is needed to make the field \(\textbf{a}\) encode this information, it is sufficient to use the initialization rule \(a_i = \log[p_i/(1-p_i)]\) as reported in \cite{Hinton_guide}. To understand how the structure of the data is encoded into the weights matrix, instead, we monitored the singular value decomposition (SVD) of \textbf{W} during the training, given by

\begin{equation}
\textbf{W} = \textbf{U\(\Sigma V^T \) } 
\end{equation}

where the columns of \textbf{U} and \textbf{V} are respectively the left and right singular vectors and \textbf{\(\Sigma\)} is the diagonal matrix of the singular values in decreasing order. 
In the context of a RBM, we can better specify the role of the SVD matrices:

\begin{itemize}
\item \textbf{U} encodes the singular vectors related to the visible layer and can therefore be visualized in the pixel space
\item  \textbf{V} is related to the hidden layer and it is a square orthogonal matrix that can be interpreted as a rotation. 
\item The singular values \( \{ {\sigma}_j \} \) contained in \textbf{\(\Sigma\)} can be thought of as scaling factors whose action is to weight the singular vectors composing \textbf{W}.
\end{itemize}

Given the above characteristics we focused our attention on \textbf{\(\Sigma\)} and \textbf{U}, tracking the distribution of the singular values and looking at the corresponding left singular vectors during the training.

\subsection{Distribution of the singular values}
The weights matrix \(\textbf{W}\) is initialized as a gaussian random matrix, whose singular values distribution is known to be given by the Marchenko-Pastur law. Fig. x shows the agreement between the empirical distribution and the theoretical distribution. In particular, we note how all \(\sigma_j\) have values below the threshold set by the Marchenko-Pastur law, forming a \textit{bulk} of singular values.

\begin{figure}
  \centering
  \begin{subfigure}{.45\linewidth}
    \includegraphics[width=\linewidth]{mp_fit.eps}
    \caption{}
  \end{subfigure}
  \begin{subfigure}{.45\linewidth}
    \includegraphics[width=\linewidth]{sv_distr_e1_b10.eps}
    \caption{}
  \end{subfigure}\par\medskip
  \begin{subfigure}{.45\linewidth}
    \includegraphics[width=\linewidth]{sv_distr_e1_b60.eps}
    \caption{}
  \end{subfigure}
  \begin{subfigure}{.45\linewidth}
    \includegraphics[width=\linewidth]{sv_distr_e1_b120.eps}
    \caption{}  
  \end{subfigure}
  \begin{subfigure}{\linewidth}
    \includegraphics[width=\linewidth]{sv_distr_e40_b600.eps}
    \caption{}  
  \end{subfigure}
 \caption{\textbf{(a)} Singular values distribution of the initial random matrix compared to Marchenko-Pastur law. \textbf{(b)-(d)} With the training we can see some singular values strengthening and overcoming the threshold set by the Marchenko-Pastur law. \textbf{(e)} Distribution of the singular values after a long training: we can see many outliers spread above threshold and a spike of below-threshold singular values near zero}
\end{figure}

Starting with the training we see that many singular values increase in value and overcome the threshold for a gaussian random matrix; these are \textit{outliers} leaving the bulk, shown in fig. x. During the first epochs of the training this process is very fast and many \(\sigma_j\) are easily extracted from the bulk, growing of many orders of magnitude. The bulk is instead shrinked to low values, meaning that the \(\sigma_j\) which do not overcome the threshold decrease in magnitude. Going on with the training this process slows down but it does not stop: outliers keep growing slowly and the bulk keeps shrinking to approach a spike around zero. It is important to note that a kind of hierarchy is maintained in the process: the first outliers are never overcome by the newly extracted \(\sigma_j\), and this is made clear by looking at the corresponding left singular vectors (see next section).
After a long training the singular values \(\sigma_j\) are separated into two categories: a concentrated set of almost-null singular values and a set of outliers spread above the threshold, as shown in fig. x.

The evolution of the \(\sigma_j\) distribution described above suggests that the training process is able to discern between the \textit{most important} singular vectors, which are brought above threshold, and a bulk of \textit{less important} singular vectors, which are practically eliminated by cutting down the corresponding \(\sigma_j\). This is a good indication about what are the dynamics of the learning process, but it is not really satisfactory. In particular, some matters need to be addressed: (i) what do singular vectors represent?, (ii) what is the meaning of \textit{more} and \textit{less} important?, (iii) when to stop the learning?

\subsection{Analysis of left singular vectors}

To understand the role of the left singular vectors of a RBM we must keep in mind the interpretation for the SVD decomposition of \textbf{W} given previously. We have seen how the matrix of the weights \textbf{\(\Sigma\)} is shaped during the learning, and we recall that \textbf{V} is interpreted as a rotation in the space of the hidden units. We then expect to recover the structure of the training data into the \textbf{U} matrix, and to show that we carefully analyze the left singular vectors during training by visualizing them in the pixel space.

Before focusing on the left singular vectors, we note that also the external visible field can be visualized in the pixel space. With the initialization rule y we are able to encode into the field the mean activation of the visible layer, which is clearly shown in fig. x in the pixel space. If we instead initialize the visible field with a null vector, the mean activation pattern is learnt very effectively as the strongest left singular value. The striking resemblance between the mean activation pattern computed from the training data and the one learnt by the RBM is shown in fig. x and it serves as a first example of what the left singular vectors represent. It seems then equivalent to either encode the mean activation pattern into the visible field since the beginning or letting the RBM learn such a pattern as a left singular vector. In practice the second case is not desirable as the RBM associates to the mean activation pattern a very strong singular value, many order of magnitudes higher then the strongest outliers. This results in a bias in the sampling from the trained machine, such that the samples whose activation pattern is nearest to the mean are sampled with a higher frequency (in the worst case those are the only configurations sampled at equilibrium).

\begin{figure}
  \centering
  \begin{subfigure}{.3\linewidth}
    \includegraphics[width=\linewidth]{mode_1_training.eps}
    \caption{}
  \end{subfigure}
  \begin{subfigure}{.3\linewidth}
    \includegraphics[width=\linewidth]{vbias.png}
    \caption{}  
  \end{subfigure}
  \begin{subfigure}{.3\linewidth}
    \includegraphics[width=\linewidth]{X_l_eigv_1.eps}
    \caption{}
  \end{subfigure}\par\medskip
  \begin{subfigure}{\linewidth}
    \includegraphics[width=.1\linewidth]{W_l_eigv_2.eps}
    \includegraphics[width=.1\linewidth]{W_l_eigv_3.eps}
    \includegraphics[width=.1\linewidth]{W_l_eigv_4.eps}
    \includegraphics[width=.1\linewidth]{W_l_eigv_5.eps}
    \includegraphics[width=.1\linewidth]{W_l_eigv_6.eps}
    \includegraphics[width=.1\linewidth]{W_l_eigv_7.eps}
    \includegraphics[width=.1\linewidth]{W_l_eigv_8.eps}
    \includegraphics[width=.1\linewidth]{W_l_eigv_9.eps}
    \includegraphics[width=.1\linewidth]{W_l_eigv_10.eps}
    \caption{}
  \end{subfigure}\par
  \begin{subfigure}{\linewidth}
    \includegraphics[width=.1\linewidth]{X_l_eigv_2.eps}
    \includegraphics[width=.1\linewidth]{X_l_eigv_3.eps}
    \includegraphics[width=.1\linewidth]{X_l_eigv_4.eps}
    \includegraphics[width=.1\linewidth]{X_l_eigv_5.eps}
    \includegraphics[width=.1\linewidth]{X_l_eigv_6.eps}
    \includegraphics[width=.1\linewidth]{X_l_eigv_7.eps}
    \includegraphics[width=.1\linewidth]{X_l_eigv_8.eps}
    \includegraphics[width=.1\linewidth]{X_l_eigv_9.eps}
    \includegraphics[width=.1\linewidth]{X_l_eigv_10.eps}
    \caption{}
  \end{subfigure}
 \caption{\textbf{(a)} First mode learnt by the RBM with the external visible field initialized as a null vector. \textbf{(b)} External visible field initialized with rule y. \textbf{(c)} First principal components extracted from the training set. \textbf{(d)} The first 9 modes of a trained RBM when rule y has been used. \textbf{(e)} Principal components extracted from the training set (starting from the second).}
\end{figure}

The first 10 left singular vectors of a trained RBM are shown in fig. x. They are all composed by a homogeneous background on the borders and a set of alternating dark and light traits in the center, highlighting the fact that each singular vector acts globally on the visible layer. Even if the pictures seen in fig. x are quite different one from another, an interesting trend is found: a higher number of alternating traits is present in the successive vectors. At this point it is useful to draw a comparison to the Fourier decomposition of an image and interpret the left singular values as the \textit{modes} composing the activation pattern. The first singular vectors, characterized by a small number of alternating traits, play the role of the \textit{low frequency} modes while the successive modes act as \textit{high frequency} modes. The analogy to Fourier modes is suggested by the fact that we are basically performing Principal Component Analysis (PCA) over the matrix \textbf{W} and the left singular vectors can thus be identified with the principal modes of variation.

Looking at the dynamics of the learning it is seen that the modes take shape one by one as the corresponding singular value \(\sigma_j\) is brought above threshold. The subsequent strengthening of the \(\sigma_j\) corresponds to refinements and rotations, with little effects on the characterization of the modes as high or low frequency modes. For what concerns the modes below threshold, they present a dark border and a random configuration in the center; in this case the only effect of the training is to discern what are the units which are never activated and no information about the actual structure of the data is found.

These observations suggest that the RBM is able to learn the modes that compose the activation patterns of the data, starting with the low frequency modes and proceeding with the high frequency ones. Moreover,with reference to the \(\sigma_j\) distribution (fig. x), we note that the low frequency modes are given a higher weigth.

Summarizing, the analysis of the left singular vectors gives many insights on the training procedure:

\begin{itemize}
\item a RBM is able to learn the modes that compose the activation patterns of the training data
\item the low frequency modes are learnt first and are given higher weigths
\item by continuing the training, new modes at increasing higher frequency are learnt while the already learnt modes are strengthen
\item the external visible field is equivalent to a left singular vector but we can avoid to learn it by using the appropriate initial conditions
\end{itemize}

[they don't change with the epochs bla bla...].

\section{Role of the modes}
By looking at the singular values distribution of \textbf{W} we have seen that there seem to be \textit{more} and \textit{less important} singular vectors. In the previous section we have then refined this observation by highlighting how the lowest-frequency modes are given the highest weights. We can then identify the more (less) important modes as the low (high) frequency ones. To gain some intuition about the meaning of this separation we can take our analogy further and think about the Fourier decomposition of a square wave; in such a case, the superposition of the low frequency harmonics is sufficient to build a good approximation of a square wave, while the role of the high frequency harmonics is that of sharpening the waveform at the discontinuity points. In the context of a trained RBM, we then expect that good approximations to the training data are obtained by exploiting only the low frequency modes, while the high frequency modes should represent minor corrections. Confirmation to our expectations is presented in fig. x, in which high frequency modes are shown to just provide boundary corrections.

\begin{figure}
  \begin{subfigure}{\linewidth}
  	\centering
    \includegraphics[width=.8\linewidth]{complete40ep.png}
    \caption{}
  \end{subfigure}\par
  \begin{subfigure}{\linewidth}
  	\centering
    \includegraphics[width=.8\linewidth]{difference20ep.png}
    \caption{}  
  \end{subfigure}\par
  \begin{subfigure}{\linewidth}
  	\centering
    \includegraphics[width=.8\linewidth]{difference40ep.png}
    \caption{}
  \end{subfigure}
  \caption{\textbf{(a)} The image shows some samples obtained with the trained RBM (after a 40 epochs training) and then "filtered" by eliminating the 400 weakest modes (just the 100 strongest modes are retained). \textbf{(b)} The images are composed by eliminating the 100 strongest modes to see what the weakest modes actually encode (20 epochs training). \textbf{(c)} As in \textbf{(b)} but after a 40 epochs training.}
\end{figure}

\input{bib.tex}

\end{document}
